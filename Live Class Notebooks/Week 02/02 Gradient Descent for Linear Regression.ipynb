{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzfPOmyKvduy"
   },
   "source": [
    "# Linear Regression (1 feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3033,
     "status": "ok",
     "timestamp": 1643090441398,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 480
    },
    "id": "imcNmFXhPdCh"
   },
   "outputs": [],
   "source": [
    "# Import our standard libraries.\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns  # for nicer plots\n",
    "sns.set(style='darkgrid')  # default style\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VloE32t7dkU1"
   },
   "source": [
    "## Data for Linear Regression\n",
    "\n",
    "Suppose we have a dataset with 2 datapoints, $x^{(0)}$ and $x^{(1)}$. Each datapoint represents the value of a single feature. Because we'll want our model to have a learned *bias* (or *intercept*), we will use an extra feature which will always be set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1643090375033,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 480
    },
    "id": "8bcduWsAbCRl"
   },
   "outputs": [],
   "source": [
    "# Here are our inputs.\n",
    "X = np.array([[1, 3],\n",
    "              [1, 2]])\n",
    "Y = np.array([7, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSWDARd9eM99"
   },
   "source": [
    "## Example slicing\n",
    "Practice slicing the input array X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1643090376249,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 480
    },
    "id": "IeQiqqBTd-t3"
   },
   "outputs": [],
   "source": [
    "# Use slicing to get the first X example (as an array)\n",
    "x_e0 = X[0,:]\n",
    "# Use slicing to get the second X example (as an array)\n",
    "x_e1 = X[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsuG8A-zeUV6"
   },
   "source": [
    "## Make predictions\n",
    "Suppose we are using linear regression. Then the functional form for our model is: \n",
    "\n",
    "\\begin{align}\n",
    "h(x) &= w_0 + w_1x \n",
    "\\end{align}\n",
    "\n",
    "Since we're using an extra feature corresponding to the intercept/bias, we can write:\n",
    "\n",
    "\\begin{align}\n",
    "h(x) &= w_0x_0 + w_1x_1 \\\\\n",
    "\\end{align}\n",
    "\n",
    "Given parameter values, practice computing model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1643090379415,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 480
    },
    "id": "HDbc2mGXcTvM",
    "outputId": "e12af905-78bf-472d-9911-b12df67d3490"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Let's use a linear regression model: f(x) = w0 + w1*x1\n",
    "W = np.array([1, 1])\n",
    "\n",
    "# Compute the prediction of the model for the first X example\n",
    "pred_e0 = np.dot(W, X[0])\n",
    "print(pred_e0)\n",
    "\n",
    "# Compute the prediction of the model for the second X example\n",
    "pred_e1 = np.dot(W, X[1])\n",
    "print(pred_e1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tdGfEoDovBm"
   },
   "source": [
    "Let's rewrite our model function with matrix multiplication and using the notation $h_W$ to make clear that our model is *parameterized* by $W$ (the vector of parameters):\n",
    "\n",
    "\\begin{align}\n",
    "h_W(x) = w_0x_0 + w_1x_1 = xW^T\n",
    "\\end{align}\n",
    "\n",
    "To make this matrix formulation as clear as possible, this is:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y} = h_W(x) = xW^T =\n",
    "\\begin{pmatrix}\n",
    "x_0 & x_1 \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "In addition, if we wanted to apply our model to *all* inputs $X$, we could simply use $XW^T$:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{Y} = h_W(X) = XW^T =\n",
    "\\begin{pmatrix}\n",
    "x_{0,0} & x_{0,1} \\\\\n",
    "x_{1,0} & x_{1,1} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "x_{m-1,0} & x_{m-1,1} \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Remember that [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication) requires the inner dimensions to line up: \n",
    "\n",
    "\\begin{align}\n",
    "X_{\\{m \\times n\\}} W^T_{\\{n \\times 1 \\}}  = \\hat{Y}_{\\{m \\times 1 \\}}\n",
    "\\end{align}\n",
    "\n",
    "Ok your turn.\n",
    "\n",
    "Use numpy functions to compute predictions for both X examples at once. The result should be a vector with 2 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1643090382824,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 480
    },
    "id": "dRyOr1S1oWAx",
    "outputId": "f0172295-52f7-4d1a-f07d-3bc8217aa187"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 3]\n"
     ]
    }
   ],
   "source": [
    "# Compute predictions for all X examples at once.\n",
    "#X: 2x2\n",
    "#W.T: 2x1 \n",
    "#Y_hat: 2x1\n",
    "preds = np.dot(X, W.T)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8ZlvnYOpWxR"
   },
   "source": [
    "## Loss\n",
    "Use numpy functions to compute a vector of differences between model predictions and labels (values of Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1643090384551,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 480
    },
    "id": "4UEd8aEbokKL",
    "outputId": "5b6a188c-2c8f-40e6-beaf-4a751f1f40dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3 -2]\n"
     ]
    }
   ],
   "source": [
    "# Compute differences between predictions and labels.\n",
    "diff = preds - Y\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J90USA7GqA2b"
   },
   "source": [
    "Now compute the MSE loss. The result should be a single (scalar) value. Remember we're using this formula (see assignment 1):\n",
    "\n",
    "\\begin{align}\n",
    "J(W) = \\frac{1}{2m} \\sum_{i=0}^{m-1} (h_W(x^{(i)}) - y^{(i)})^2\n",
    "\\end{align}\n",
    "\n",
    "where we've changed the standard scaling from $\\frac{1}{m}$ to $\\frac{1}{2m}$, where $m$ is the number of examples (2, in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 198,
     "status": "ok",
     "timestamp": 1643090534135,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 480
    },
    "id": "TcjG93R6qe-H",
    "outputId": "e7bee7a8-3968-4b11-8b36-e4e674a313d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.5\n"
     ]
    }
   ],
   "source": [
    "# Get the number of examples\n",
    "m = X.shape[0]\n",
    "\n",
    "# Compute the average per-example loss\n",
    "loss = np.sum(diff**2) / m\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWtjfoCZ5n1f"
   },
   "source": [
    "## Gradient\n",
    "\n",
    "Refer to assignment 1 or the gradient descent lecture for the derivation, but here's the formula for the partial derivatives (which together form the gradient):\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial w_j} J(W) &= (h_W(x) - y)x_j\n",
    "\\end{align}\n",
    "\n",
    "This formula is assuming we only have a single example. The general formula has an average over examples:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial w_j} J(W) &= \\frac{1}{m}\\sum_i(h_W(x^{(i)}) - y^{(i)})x^{(i)}_j\n",
    "\\end{align}\n",
    "\n",
    "You're ready to compute the gradient. The result will be a vector of partial derivatives for $w_0$ and $w_1$ respectively. You can use matrix computations as before.\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla J(W) &= \\frac{1}{m}(h_W(X) - Y)X\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 166,
     "status": "ok",
     "timestamp": 1643090387372,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 480
    },
    "id": "EwxFEpUr5q9V",
    "outputId": "5e4cc30e-880f-4458-d5be-5fd53a5a8d90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.5 -6.5]\n"
     ]
    }
   ],
   "source": [
    "# compute the gradient\n",
    "gradient = np.dot(diff, X) / m\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpBEnP5sLZks"
   },
   "source": [
    "## Parameter updates\n",
    "Now that you've computed the gradient, apply parameter updates by subtracting the appropriate partial derivatives (scaled by a learning rate) from the initial parameter values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1643090389127,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 480
    },
    "id": "5dtpDEhYZZwg",
    "outputId": "48c10deb-a0b0-4d07-ea03-1791602046c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.25 1.65]\n"
     ]
    }
   ],
   "source": [
    "# Update parameter values\n",
    "learning_rate = 0.1\n",
    "W = W - learning_rate*gradient\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff3KjlpLvsbG"
   },
   "source": [
    "# Linear Regression (multiple features)\n",
    "\n",
    "Suppose we have a dataset with 2 datapoints, $x^{(0)}$ and $x^{(1)}$, but now we have multiple features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 163,
     "status": "ok",
     "timestamp": 1643090391819,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 480
    },
    "id": "CVGGtopiv39T"
   },
   "outputs": [],
   "source": [
    "# Here are our inputs.\n",
    "X = np.array([[1, 3, 1, 1],\n",
    "              [1, 2, 2, 0]])\n",
    "Y = np.array([2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4bG4AeHv39T"
   },
   "source": [
    "Let's write out our model function:\n",
    "\n",
    "\\begin{align}\n",
    "h_W(x) = w_0x_0 + w_1x_1 + w_2x_2 + w_3x_3 = xW^T\n",
    "\\end{align}\n",
    "\n",
    "We can get all predictions with this matrix product:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{Y} = h_W(X) = XW^T =\n",
    "\\begin{pmatrix}\n",
    "x_{0,0} & x_{0,1} & x_{0,2} & x_{0,3} \\\\\n",
    "x_{1,0} & x_{1,1} & x_{1,2} & x_{1,3} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "x_{m-1,0} & x_{m-1,1} & x_{m-1,2} & x_{m-1,3} \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "w_3 \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Given the (initial) parameter values below, compute the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 168,
     "status": "ok",
     "timestamp": 1643090393439,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 480
    },
    "id": "pGg1Ll4I4jR6",
    "outputId": "d404bdac-1104-4983-b597-cf0f03247e5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 5]\n"
     ]
    }
   ],
   "source": [
    "# Initial parameter values.\n",
    "W = [1, 1, 1, 1]\n",
    "\n",
    "# Compute predictions.\n",
    "preds = np.dot(X, W)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZ_pD-j2_qzV"
   },
   "source": [
    "Now compute the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 163,
     "status": "ok",
     "timestamp": 1643090395232,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 480
    },
    "id": "T-Sam58B8B1_",
    "outputId": "0bd0b1d1-715c-4fc6-911e-4e8220abb223"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4. 10.  6.  2.]\n"
     ]
    }
   ],
   "source": [
    "m, n = X.shape\n",
    "gradient = np.dot((preds - Y), X) / m\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmYUmEjg_sk8"
   },
   "source": [
    "And now put everything together in gradient descent. You can run this cell repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 153,
     "status": "ok",
     "timestamp": 1643090413994,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 480
    },
    "id": "Zl_Nu_wB8ar4",
    "outputId": "6c196a39-9200-43e3-e090-80950c76373b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: [6 5]\n",
      "loss: 16.0\n",
      "gradient: [ 8. 20. 12.  4.]\n",
      "weights: [ 0.2 -1.  -0.2  0.6]\n"
     ]
    }
   ],
   "source": [
    "# Run gradient descent\n",
    "learning_rate = 0.1\n",
    "\n",
    "preds = np.dot(X, W)\n",
    "loss = ((preds - Y)**2).mean()\n",
    "gradient = 2 * np.dot((preds - Y), X) / m\n",
    "W = W - learning_rate * gradient\n",
    "\n",
    "print('predictions:', preds)\n",
    "print('loss:', loss)\n",
    "print('gradient:', gradient)\n",
    "print('weights:', W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMsSJ-ZD_12e"
   },
   "source": [
    "## Now with TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 344,
     "status": "ok",
     "timestamp": 1643090447058,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 480
    },
    "id": "jisaFtGY__KL"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    units=1,                     # output dim\n",
    "    input_shape=[4],             # input dim\n",
    "    use_bias=False,              # we included the bias in X\n",
    "    kernel_initializer=tf.ones_initializer,  # initialize params to 1\n",
    "))\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "model.compile(loss='mse', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1316,
     "status": "ok",
     "timestamp": 1643090453285,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 480
    },
    "id": "3PQ-RDwXCKVt",
    "outputId": "ca4ff499-ec50-4afc-d654-94747a8d6a59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 60ms/step\n",
      "predictions: [[-2.4 -2.2]]\n",
      "loss: 16.0\n",
      "W: [[ 0.19999999 -1.         -0.20000005  0.6       ]]\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "  x = X,\n",
    "  y = Y,\n",
    "  epochs=1,\n",
    "  batch_size=2,\n",
    "  verbose=0)\n",
    "loss = history.history['loss'][0]\n",
    "weights = model.layers[0].get_weights()[0].T\n",
    "preds = model.predict(X)\n",
    "\n",
    "print('predictions:', preds.T)\n",
    "print('loss:', loss)\n",
    "print('W:', weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-QZEAyrD0pyz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNOgo8Tn1aaBqOYARTcgIQP",
   "collapsed_sections": [],
   "name": "01 Gradient Descent for Linear Regression.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
